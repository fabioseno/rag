# rag

RAG (Retrieval-Augmented Generation) in action.

## About this project

This sample intends to show some basic AI concepts related to RAG.

## Features

[X] Load an run LLM models locally using [Ollama](https://ollama.com/)
[X] Transform texts into vectors (array of numbers) in a process called Embedding
[X] Store vectors in a database with vector support like PosgreSQL/pgvector
[X] Query vectors by similarity
[X] Ask a question related to these texts and use an AI model to describe the best result using different words

## Local configuration
- install and run Docker
- run ```npm install```
- run ```docker compose up```
- in another console, run ```ollama serve```

## Learning notes

- LLM models vary in number of parameters from millions to trillions. The bigger the # of parameters, the more processing power it will require to perform calculations, but the results tend to be more accurate.
- vectors generated by different LLM models will result in arrays with different sizes. This affects the column definition in the database.

    | LLM model        | Data type    |
    | ---------------- | ------------ |
    | nomic-embed-text | vector(768)  |
    | llama3.2:3b      | vector(3072) |
    | all-minilm:33m   | vector(384)  |
    | tinyllama        | vector(2048) |
    | mistral:7b       | vector(4096) |
